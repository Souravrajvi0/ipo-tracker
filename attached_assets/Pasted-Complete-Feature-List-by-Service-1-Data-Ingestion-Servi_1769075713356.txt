Complete Feature List (by Service)
1. Data Ingestion Service
âœ… Daily cron scrape Chittorgarh /ipo/ipo_dashboard.asp
â€‹

âœ… Parse: symbol, company name, open/close dates, price band, lot size, min qty

âœ… Extract GMP (grey market premium) from live bidding page
â€‹

âœ… Parse DRHP PDFs â†’ extract financials (3Y revenue, EBITDA, PAT, margins)

âœ… Deduplicate entries (Redis cache IPO:${symbol})

âœ… Error handling + retry logic (BullMQ, 3 attempts)

âœ… Rate limiting (10 scrapes/min, respect robots.txt)

âœ… Slack alert on scrape failures

2. Financial Analysis Service
âœ… Extract from DRHP:

Revenue (3Y), EBITDA, PAT, EPS

Margins: EBITDA%, PAT%

ROE, ROCE (if disclosed)

Debt/EBITDA, Interest coverage

Capex plans, cash flows

Promoter holding, pledge status

âœ… Normalize for missing data (interpolate, flag)

âœ… Store in PostgreSQL (schema: 40+ columns)

3. Scoring Engine (Rule-Based)
Compute score: 0-10 with breakdown:

Category	Metrics	Weight	Formula
Fundamentals	Rev CAGR 3Y, PAT consistency, ROE	35%	CAGR>20?:8 CAGR>10?:5 :2
Valuation	P/E, P/B vs sector median	30%	PE_ratio vs peers â†’ 3-8
Debt Health	Debt/EBITDA, interest coverage	15%	<3 EBITDA?:7 :3
Growth Potential	Capex/revenue, TAM, use of funds	15%	Strong capex?:7 :4
Risk Flags	High OFS %, promoter exit, litigation	-20%	Each flag: -5 to -20
Output JSON:

json
{
  "symbol": "XYZ",
  "total_score": 7.2,
  "scores": {
    "fundamentals": 7,
    "valuation": 5,
    "debt": 8,
    "growth": 7
  },
  "red_flags": ["OFS: 60% (promoter exit)", "P/E: 25x vs median 15x"],
  "suitability": "Moderate Risk"
}
4. AI Analysis Service
âœ… Feed structured data + DRHP excerpts to OpenAI

âœ… Function calling for structured output:

analyzeMoat() â†’ "Strong brand + network effects" OR "commodity, high competition"

assessRisks() â†’ ["high debt maturity", "regulatory risk", "customer concentration"]

evaluateManagement() â†’ promoter track record assessment

competitorComparison() â†’ "vs XYZ (listed), this is 20% cheaper"

âœ… Output: Markdown narrative (500 words max)

âœ… Cache responses (same metrics â†’ same analysis)

Example output:

text
**Moat Analysis**: Strong IP portfolio + 80% gross margins suggest pricing power.

**Key Risks**: 
- Concentration: Top 3 customers = 45% revenue
- Commodity risk: Steel prices 2023-2024 volatility

**Valuation**: Premium to peers justified by ROCE. Manageable.

**Bottom Line**: Quality business at fair price. Wait 3-6m for clarity.
5. Portfolio Tracking Service
âœ… Manual input: CSV upload {symbol, lots_applied, allotted_qty, buy_price}

âœ… Auto-fetch CMP from NSE/YahooFinance daily

âœ… Calculate: cost, current value, P&L %, return since listing

âœ… Aggregate: portfolio return vs Nifty50, hit rate (% winners)

âœ… Alert: "ABC IPO +30%, lock â‚¹50k profit?"

Dashboard table:

text
Symbol | Allot | Cost | CMP | +1M% | P&L | Status
XYZ    | 10    | â‚¹500 | â‚¹680 | +15% | +36% | Hold
ABC    | 5     | â‚¹100 | â‚¹85  | -8%  | -15% | Stop-loss
6. Backtesting Service
âœ… Scrape 500+ historical IPOs (2020-2025) from Chittorgarh

âœ… Retroactively compute scores using your rules

âœ… Compare: score threshold vs actual 1M/6M/1Y returns

âœ… Output: "Score >7 = 68% hit rate, avg +18% listing day"

âœ… API: GET /backtest?min_score=7&time_horizon=1M â†’ metrics

Example:

json
{
  "threshold": 7.0,
  "sample_size": 145,
  "hit_rate": 0.68,
  "avg_listing_gain": 18.5,
  "avg_6m_return": 22.0,
  "win_loss_ratio": 1.8
}
7. Alert & Notification Service
âœ… Telegram bot:

"New IPO: ABC (ACME Corp) opens tomorrow"

"Score: 7.8/10 | Moat: Strong | Risk: Moderate"

"Your score threshold: 7.5 â†’ APPLY? Yes/No"

âœ… Email digest: Weekly summary (top 3 upcoming)

âœ… Portfolio alerts:

"XYZ listing today, GMP â‚¹150 â†’ expected pop +30%"

"ABC P&L +25%, consider partial exit"

8. API & Caching Layer
Express endpoints:

Endpoint	Method	Params	Response
/api/ipos/upcoming	GET	-	Array of next 10 IPOs
/api/ipos/:symbol/score	GET	-	Full scoring breakdown
/api/ipos/:symbol/analysis	GET	-	AI narrative + risks
/api/ipos/:symbol/compare	GET	industry	Peer benchmarks
/api/portfolio	GET	-	Holdings + P&L
/api/portfolio/add	POST	{symbol, qty, price}	Add holding
/api/backtest	GET	?min_score=7&horizon=1M	Hit rate metrics
/api/alerts/telegram/subscribe	POST	{chat_id, min_score}	Subscribe
Redis caching:

IPO data: 6h TTL

Scores: 24h TTL

AI analysis: 7d TTL

CMP prices: 1h TTL

9. Monitoring & Health Checks
âœ… Health endpoint: GET /health â†’ scraper status, last run, DB rows

âœ… Logs: All scrapes, scores, AI calls logged to PostgreSQL

âœ… Alerts: Scraper down >2h? Slack to you

âœ… Metrics: "5 new IPOs found", "3 scored >7", "2 AI analyses"

Feature Prioritization (MVP vs Nice-to-Have)
Phase 1 (Week 1-2) â€” MVP
DataSvc: Chittorgarh scrape â†’ Postgres âœ…

ScoringSvc: 4 simple rules â†’ score âœ…

Telegram basic alerts âœ…

Docker setup âœ…

Phase 2 (Week 3) â€” Core Differentiation
AI Service (OpenAI function calling) âœ…

Portfolio tracking âœ…

Express API (/upcoming, /score, /analysis) âœ…

Phase 3 (Week 4+) â€” Advanced
Backtesting engine âœ…

Peer comparison âœ…

Email digests, advanced alerts âœ…

Historical performance tracking (post-listing) âœ…

Tech Stack Summary
text
Backend: Node.js 20 + Express.js
Database: PostgreSQL 16
Queue: BullMQ (Redis)
Scraping: Puppeteer + Cheerio + pdf-parse
AI: OpenAI gpt-4o-mini (function calling)
Alerts: Telegram Bot API + optional Nodemailer
Caching: Redis (6h-7d TTLs)
Deploy: Docker + docker-compose
Monitoring: Winston logs + Slack
This is your complete spec. Start with Phase 1 (3-4 days), then iterate. Each feature is interview-gold: ETL, rule engines, API design, caching, async jobs.

Which phase should we dive into first?

Prepared using Deep research
so now for this these services should be in one project or these are like different projects all togehter
Single monolithic project with modular services folder structureâ€”best for your SDE prep and personal use.

Recommended: Monolith with Service Modules
One repo, organized by concern:

text
ipo-analyzer/
â”œâ”€â”€ docker-compose.yml          # Postgres + Redis
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ services/               # 7 core services
â”‚   â”‚   â”œâ”€â”€ data-ingestion/     # Scraper + parser
â”‚   â”‚   â”‚   â”œâ”€â”€ chittorgarh-scraper.js
â”‚   â”‚   â”‚   â”œâ”€â”€ drhp-parser.js
â”‚   â”‚   â”‚   â””â”€â”€ db-loader.js
â”‚   â”‚   â”œâ”€â”€ financial-analysis/  # Extract + normalize
â”‚   â”‚   â”‚   â”œâ”€â”€ financial-extractor.js
â”‚   â”‚   â”‚   â””â”€â”€ validation.js
â”‚   â”‚   â”œâ”€â”€ scoring-engine/      # Rule-based scoring
â”‚   â”‚   â”‚   â”œâ”€â”€ scorer.js
â”‚   â”‚   â”‚   â””â”€â”€ rules.js
â”‚   â”‚   â”œâ”€â”€ ai-analysis/         # OpenAI service
â”‚   â”‚   â”‚   â”œâ”€â”€ ai-analyzer.js
â”‚   â”‚   â”‚   â””â”€â”€ prompts.js
â”‚   â”‚   â”œâ”€â”€ portfolio-tracker/   # Holdings + P&L
â”‚   â”‚   â”‚   â”œâ”€â”€ portfolio-service.js
â”‚   â”‚   â”‚   â””â”€â”€ cmp-fetcher.js
â”‚   â”‚   â”œâ”€â”€ backtest-engine/     # Historical validation
â”‚   â”‚   â”‚   â”œâ”€â”€ backtest.js
â”‚   â”‚   â”‚   â””â”€â”€ metrics.js
â”‚   â”‚   â””â”€â”€ alerts/              # Telegram + Email
â”‚   â”‚       â”œâ”€â”€ telegram-bot.js
â”‚   â”‚       â””â”€â”€ email-service.js
â”‚   â”œâ”€â”€ api/                     # Express routes
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ ipos.js
â”‚   â”‚   â”‚   â”œâ”€â”€ portfolio.js
â”‚   â”‚   â”‚   â”œâ”€â”€ backtest.js
â”‚   â”‚   â”‚   â””â”€â”€ health.js
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”‚       â”œâ”€â”€ auth.js
â”‚   â”‚       â”œâ”€â”€ cache.js
â”‚   â”‚       â””â”€â”€ error-handler.js
â”‚   â”œâ”€â”€ db/                      # Database
â”‚   â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â”‚   â”œâ”€â”€ 001_create_ipos.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 002_create_scores.sql
â”‚   â”‚   â”‚   â””â”€â”€ 003_create_portfolio.sql
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ ipo.js
â”‚   â”‚       â”œâ”€â”€ score.js
â”‚   â”‚       â””â”€â”€ portfolio.js
â”‚   â”œâ”€â”€ jobs/                    # Cron jobs
â”‚   â”‚   â”œâ”€â”€ daily-scrape.js
â”‚   â”‚   â”œâ”€â”€ score-new-ipos.js
â”‚   â”‚   â””â”€â”€ update-portfolio.js
â”‚   â”œâ”€â”€ utils/                   # Shared helpers
â”‚   â”‚   â”œâ”€â”€ logger.js
â”‚   â”‚   â”œâ”€â”€ redis-client.js
â”‚   â”‚   â””â”€â”€ openai-client.js
â”‚   â””â”€â”€ app.js                   # Express setup
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/
â”‚   â””â”€â”€ jobs/
â”œâ”€â”€ .github/workflows/           # CI/CD (optional)
â””â”€â”€ README.md
Why Monolith (Not Microservices)
Aspect	Monolith	Microservices
Deployment	docker-compose up (1 command)	7 services to manage separately
Development	Single repo, easy iteration	Complex cross-service debugging
Scaling	Good enough for personal use	Overkill for your scope
Interview Value	"Designed modular monolith" (strong)	"Distributed systems" (overengineered for project scale)
Ops Complexity	Docker + Postgres + Redis	+service discovery, load balancing
Database	Shared Postgres + Redis	Per-service DBs? Data sync nightmare
Latency	HTTP calls within process	Network hops between services
For your case: Monolith wins. You're optimizing for SDE prep, not Netflix scale.

Service Interaction Pattern (Single Process)
javascript
// src/app.js
const express = require('express');
const dataService = require('./services/data-ingestion');
const scoringService = require('./services/scoring-engine');
const aiService = require('./services/ai-analysis');
const portfolioService = require('./services/portfolio-tracker');
const alertService = require('./services/alerts');

const app = express();

// All services run in same Node process
// Call them directly: no HTTP overhead

app.get('/api/ipos/:symbol/score', async (req, res) => {
  const ipoData = await dataService.getIPO(req.params.symbol);
  const score = await scoringService.score(ipoData);
  const analysis = await aiService.analyze(ipoData);
  
  res.json({ score, analysis });
});
Deployment Story (Interview-Ready)
bash
# Clone + deploy in 2 commands
git clone https://github.com/you/ipo-analyzer
cd ipo-analyzer && docker-compose up

# Services communicate in-process
# Logs visible in docker output
# Postgres accessible: localhost:5432
# Redis: localhost:6379
# API: localhost:3000
When to Split (Future Scale)
Stay monolith unless:

Scraper needs independent scaling (heavy I/O) â†’ separate worker pod

AI service expensive (latency) â†’ async queue + separate consumer

Portfolio tracking hit by many users â†’ API service separate

But for personal use + SDE interview? Monolith is cleaner, faster, and more impressive ("I built a complete system from ETL to API in one codebase").

Code Example: Single Service Call
javascript
// src/services/data-ingestion/chittorgarh-scraper.js
async function scrapeIPOs() {
  console.log('ðŸ” Starting daily Chittorgarh scrape...');
  const ipos = await fetchChittorgarhPage();
  const enriched = await Promise.all(ipos.map(async (ipo) => {
    return {
      ...ipo,
      financials: await extractDRHPFinancials(ipo.drhpUrl),
      gmp: await fetchGMP(ipo.symbol)
    };
  }));
  await db.ipos.bulkCreate(enriched);
  return enriched;
}

// Used in jobs and API endpoints same way
module.exports = { scrapeIPOs };
Folder Access Pattern
All services import from shared utils:

javascript
// Services access DB, Redis, Logger centrally
const db = require('../db/models');
const redis = require('../utils/redis-client');
const logger = require('../utils/logger');

// One source of truth
logger.info('IPO scored', { symbol: 'ABC', score: 7.5 });
await redis.setex(`IPO:ABC`, 86400, JSON.stringify(data));
Setup (Day 1)
bash
npm init -y
npm install express sequelize pg redis bull @anthropic-ai/sdk puppeteer cheerio
mkdir -p src/{services,api,db,jobs,utils}
docker-compose up -d

# Services ready to build modularly
Recommendation: Build as single monolith, organize by service concern. Interview story: "Built a complete IPO analysis backendâ€”ETL pipeline, scoring engine, AI insights, portfolio trackingâ€”all in one Node.js codebase with Postgres and Redis. Scraped 1000+ IPOs, achieved 68% hit rate."

Ready to start Phase 1?